<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>How to fine-tune BERT with HuggingFace – tutorials</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {  background-color: #f8f8f8; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ef2929; } /* Alert */
    code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #c4a000; } /* Attribute */
    code span.bn { color: #0000cf; } /* BaseN */
    code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4e9a06; } /* Char */
    code span.cn { color: #000000; } /* Constant */
    code span.co { color: #8f5902; font-style: italic; } /* Comment */
    code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
    code span.dt { color: #204a87; } /* DataType */
    code span.dv { color: #0000cf; } /* DecVal */
    code span.er { color: #a40000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #0000cf; } /* Float */
    code span.fu { color: #000000; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
    code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
    code span.ot { color: #8f5902; } /* Other */
    code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
    code span.sc { color: #000000; } /* SpecialChar */
    code span.ss { color: #4e9a06; } /* SpecialString */
    code span.st { color: #4e9a06; } /* String */
    code span.va { color: #000000; } /* Variable */
    code span.vs { color: #4e9a06; } /* VerbatimString */
    code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="how-to-fine-tune-bert-using-huggingface">How to fine-tune BERT using HuggingFace</h1>
<p>Joe Cummings</p>
<p>21 July, 2021 (Last Updated: 21 July, 2021)</p>
<hr />
<ul>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#background">Background</a></li>
<li><a href="#fine-tune">Fine-tuning BERT</a>
<ul>
<li><a href="#setup">Setup</a></li>
<li><a href="#preprocessing">Preprocessing</a></li>
<li><a href="#training-eval">Training &amp; Evaluation</a></li>
</ul></li>
</ul>
<hr />
<h3 id="motivation"><strong>Motivation</strong></h3>
<p>BERT is built upon a machine learning architecture called a Transformer and Transformers are sick and tight. Also, everyone from those just flirting with NLP to those on the cutting edge will have to use a Transformer-based model at some point in their lives.</p>
<blockquote>
<p>I’d highly recommend reading through this entire post as it adds color to the model you’ll be building, but if you just want the TL;DR, you can skip to the <a href="#fine-tune">tutorial part</a> now.</p>
</blockquote>
<h3 id="background"><strong>Background</strong></h3>
<p>The Transformer architecture was introduced in the paper <a href="https://arxiv.org/abs/1706.03762">“Attention Is All You Need”</a> in 2017 and has since been cited over 24k times. The Transformer has proven to be both superior in quality and faster to train by virtue of relying solely on <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">attention mechanisms</a> - doing away with cumbersome convolution and recurrence. I’d highly recommend reading more about it before going further with BERT - see these amazing resources <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">here</a> and <a href="https://jalammar.github.io/illustrated-transformer/">here</a>.</p>
<blockquote>
<p>Extra challenge: please <a href="mailto:">email me</a> if you have an intuitive way to explain <strong>positional encoding</strong> because it still trips me up sometimes.</p>
</blockquote>
<p>Researchers jumped at the chance to build upon the Transformer architecture and soon the world had The Allen Institute’s <a href="https://allennlp.org/elmo">ELMo</a> and OpenAI’s <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT/GPT-2</a>. For this tutorial, we’ll look at Google’s <a href="https://arxiv.org/abs/1810.04805">BERT</a>, which was introduced in 2019.</p>
<p>So, what sets BERT apart? The answer lies in the name: <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers. Previous models, like GPT, encoded sequences using a left-to-right architecture, meaning that every token can only “see” the tokens that came before it. This is sub-optimal for tasks like question answering where it is extremely helpful to integrate context from the entire sequence. BERT’s architecture enables deep bidirectional representations.</p>
<figure>
<img src="https://jc-tutorials.s3.us-east-2.amazonaws.com/fine-tune-bert/self-attention-and-masked-self-attention.png" alt="Comparison of self-attention - what BERT uses - and masked attention - what GPT uses (source)" /><figcaption aria-hidden="true"><em>Comparison of self-attention - what BERT uses - and masked attention - what GPT uses (<a href="https://jalammar.github.io/illustrated-gpt2/">source</a>)</em></figcaption>
</figure>
<p>You may be thinking at this point: “Okay, why mask the attention mechanism? Why not just integrate the context from the entire sequence from the start?”. The answer is that strictly bidirectional conditioning would allow each token in a sequence to essentially “see itself” and the output would be trivially predicted. Imagine I gave you the following sentence: “The dog went to the park” and asked you to “predict” what word came after “dog”. Since you have the entire sentence as context, you <em>know</em> that “went” immediately succeeds “dog”. While this is a slight oversimplification, it should convey the general idea. The diagram below also helps visualize the difference between these langauge modeling methods.</p>
<figure>
<img src="https://jc-tutorials.s3.us-east-2.amazonaws.com/fine-tune-bert/model-comparisons.png" alt="Encoding styles of BERT, GPT, and ELMo. ELMo does a shallow concatenation of a left-to-right encoding and a right-to-left encoding.(source)" /><figcaption aria-hidden="true"><em>Encoding styles of BERT, GPT, and ELMo. ELMo does a shallow concatenation of a left-to-right encoding and a right-to-left encoding.(<a href="https://arxiv.org/abs/1810.04805">source</a>)</em></figcaption>
</figure>
<p>So, if bidirectional encoding is impossible, how is BERT doing it? BERT introduces something called a <strong>“masked language model”</strong> (MLM), but you might also see this referrred to as a <a href="https://aclanthology.org/W10-1007.pdf">cloze</a> task. In pre-training, 15% of all tokens are replaced with a special <code>[MASK]</code> token or a random token.</p>
<pre><code>The dog went to the park. -&gt; The dog [MASK] to the park.
                          -&gt; The dog banana to the park.</code></pre>
<p><em>Example of how the sequence “The dog went to the park” would be masked in pre-training of BERT.</em></p>
<p>The model then is tasked with predicting the correct missing token. So rather than processing the left context of a sequence and trying predict the next token, BERT has to learn how to predict at random spots in the sentence.</p>
<p>While MLM models the relationship between tokens in a sequence, BERT is also trained on with something called <strong>“next sentence prediction”</strong>, which models the relationships between sentences. This is very useful for question answering, summarization, and multiple-choice tasks. The data is encoded as shown below.</p>
<pre><code>A: The dog went to the park. 
B: It rolled around in the grass.
Classification: IsNext
---
A. The dog went to the park.
B: The crow caws at midnight.
Classification: NotNext</code></pre>
<p>These two tasks were trained with 800M words from the <a href="https://arxiv.org/abs/1805.10956">BooksCorpus</a> and the entirety of English Wikipedia, made up of over 2,500M words. Together, these make up the amazing model that is BERT.</p>
<p>Enough background - let’s get to using BERT!</p>
<h3 id="fine-tune"><strong>Fine-tuning BERT</strong></h3>
<p>There are many different ways in which we could load the BERT model and fine-tune it on a downstream task. Some excellent tutorials with different frameworks can be found below:</p>
<ul>
<li>Tensorflow: <a href="https://www.tensorflow.org/official_models/fine_tuning_bert">Fine-tuning a BERT model</a></li>
<li>PyTorch: <a href="https://mccormickml.com/2019/07/22/BERT-fine-tuning/">BERT Fine-Tuning Tutorial with PyTorch</a></li>
</ul>
<p>For this tutorial, we’ll be using the popular Transformers library from <a href="https://huggingface.co">HuggingFace</a> to fine-tune BERT on a sentiment analysis task. Despite the slightly silly name, HuggingFace is a fantastic resource for those in NLP engineering. To start, let’s create a <a href="https://docs.conda.io/en/latest/">conda environment</a> and install the HuggingFace library. To support the HuggingFace library, you’ll also need to download PyTorch.</p>
<h4 id="setup"><strong>1. Setup</strong></h4>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> create env <span class="at">--name</span> fine-tune-bert python=3.7</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate fine-tune-bert</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install transformers</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch</span></code></pre></div>
<p>In addition, we’ll need to download HuggingFace’s Datasets package, which offers easy access to many benchmark datasets. Later on, we’re also going to want to specify our own evaluation metric and for that, we need to use <a href="https://scikit-learn.org/stable/">scikit-learn</a>’s library, so go ahead and install that, too.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install datasets</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install sklearn</span></code></pre></div>
<p>Now that we’ve got everything installed, let’s start the actual work. First, let’s import the necessary functions and objects.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset, load_metric</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    BertForSequenceClassification,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    BertTokenizer,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    Trainer,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    TrainingArguments,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>You’ll notice that we are importing a version of BERT called <code>BertForSequenceClassification</code>. HuggingFace offers several versions of the BERT model including a base <code>BertModel</code>, <code>BertLMHeadMoel</code>, <code>BertForPretraining</code>, <code>BertForMaskedLM</code>, <code>BertForNextSentencePrediction</code>, <code>BertForMultipleChoice</code>, <code>BertForTokenClassification</code>, <code>BertForQuestionAnswering</code>, and more. The only real difference between a lot of these is the extra layer on top of the pretrained model which is task-specific. You can find all of those models and their specifications <a href="https://huggingface.co/transformers/model_doc/bert.html">here</a>. We’re using <code>BertForSequenceClassification</code> because we are trying to classify a sequence of text with a certain emotion/sentiment.</p>
<p>The dataset we’re using is called <code>"emotion"</code> on HuggingFace’s Datasets catalog and consists of 20k Tweets labeled for one of 8 emotions: anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. You can read more details about how the data was collected, different baseline experiments, and the data distribution from the <a href="https://aclanthology.org/D18-1404/">paper</a>. So let’s load in the <code>emotions</code> dataset.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>emo_dataset <span class="op">=</span> load_dataset(<span class="st">&quot;emotion&quot;</span>)  <span class="co"># It really is that easy.</span></span></code></pre></div>
<p>Take a peek at the first 5 items in the training data and see what we have.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> emo_dataset[<span class="st">&quot;train&quot;</span>][:<span class="dv">5</span>]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;text&#39;</span>: [</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;i didnt feel humiliated&#39;</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake&#39;</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;im grabbing a minute to post i feel greedy wrong&#39;</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;i am ever feeling nostalgic about the fireplace i will know that it is still on the property&#39;</span>,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;i am feeling grouchy&#39;</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    ], </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;label&#39;</span>: [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>It appears the text has already been lowercased (good!), links and hastags have been removed, and contractions are standardized. It’s also good to double check that the data makes sense. The labels have already been converted into a numeric value, with each number corresponding to an emotion. For example, <code>0</code> is “sadness”, <code>3</code> is “anger”, and <code>2</code> is “love”.</p>
<h4 id="preprocessing"><strong>2. Preprocessing</strong></h4>
<p>Now that we have some data, we need to do some preprocessing to it so that BERT can understand and thankfully, HuggingFace provides a helpful <code>BertTokenizer</code> that takes care of this for us.</p>
<p>We can load the BERT Tokenizer from a pretrained model (they come together).</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">&quot;bert-base-uncased&quot;</span>)</span></code></pre></div>
<p>And let’s see how this is encoded!</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> tokenizer([<span class="st">&quot;The dog went to the park.&quot;</span>], padding<span class="op">=</span><span class="st">&quot;max_length&quot;</span>, truncate<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;input_ids&#39;</span>: [[<span class="dv">101</span>, <span class="dv">1996</span>, <span class="dv">3899</span>, <span class="dv">2253</span>, <span class="dv">2000</span>, <span class="dv">1996</span>, <span class="dv">2380</span>, <span class="dv">1012</span>, <span class="dv">102</span>, <span class="dv">0</span>, ..., <span class="dv">0</span>]], </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;token_type_ids&#39;</span>: [[<span class="dv">0</span>, ..., <span class="dv">0</span>]], </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;attention_mask&#39;</span>: [[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, ..., <span class="dv">0</span>]]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>At this point, you might be thinking what the hell am I looking at? Well I’ve spared your eyes by truncating the number of zeros shown in the output, but the length of each item in the returned dictionary is 512, which is the maximum length that BERT can accept. To some, this might be confusing - why do we need every input sequence to be the same size? The answer is efficiency. Linear combinations are much faster than normal multiplication and for those to be possible, all vectors need to be of the same length.</p>
<ul>
<li><code>input_ids</code> correspond to a given token in the vocabulary. So the mapping looks like the following:</li>
</ul>
<table>
<thead>
<tr class="header">
<th>token</th>
<th style="text-align: left;">input_id</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[CLS]</td>
<td style="text-align: left;">101</td>
</tr>
<tr class="even">
<td>the</td>
<td style="text-align: left;">1996</td>
</tr>
<tr class="odd">
<td>dog</td>
<td style="text-align: left;">3899</td>
</tr>
<tr class="even">
<td>went</td>
<td style="text-align: left;">4</td>
</tr>
<tr class="odd">
<td>to</td>
<td style="text-align: left;">2000</td>
</tr>
<tr class="even">
<td>park</td>
<td style="text-align: left;">2380</td>
</tr>
<tr class="odd">
<td>.</td>
<td style="text-align: left;">1012</td>
</tr>
<tr class="even">
<td>[SEP]</td>
<td style="text-align: left;">102</td>
</tr>
</tbody>
</table>
<ul>
<li><code>token_type_ids</code> are used in any tasks that contain two sequences, like question answering, summarization, etc. Because we are doing a sequence classification task with only one sentence, all our <code>token_type_ids</code> will be <code>0</code>.</li>
<li><code>attention_mask</code> refers to which tokens the model should “attend”. For the most basic case, we want the model to be able to “see” all of our tokens, which are marked with a <code>1</code>.</li>
</ul>
<p>Keep in mind while debugging, that you may see more <code>input_ids</code> than tokens you input. That’s because BERT tokenizes using <a href="">WordPiece</a> which can split some words into two or three different tokens.</p>
<p>So, how can we apply this tokenize function across all text labels? One way would simply be to iterate programmatically over every entry in the dataset and convert the text like so:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> example <span class="kw">in</span> emo_dataset[<span class="st">&quot;train&quot;</span>]:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    tokenized_text <span class="op">=</span> tokenizer(example[<span class="st">&quot;text&quot;</span>])</span></code></pre></div>
<p>Obviously, this is a very slow process and we do have a better option. HuggingFace provides us with a useful <a href="https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"><code>map</code></a> function on the <code>Dataset</code> object.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_go_emotion(example):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(example[<span class="st">&quot;text&quot;</span>], padding<span class="op">=</span><span class="st">&quot;max_length&quot;</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>tokenized_data <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_go_emotion, batched<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>In running some experiments, we can see just how much faster <code>Dataset.map</code> can be.</p>
<blockquote>
<p>Bonus question: what’s a reason we might not want to just define our function as <code>lambda x: tokenizer(x["text"])</code> and save a couple lines of code? (See answer at the end of the tutorial).</p>
</blockquote>
<p>As mentioned, <code>emotions</code> is a rather large dataset and I don’t know about you, but I’m trying not to beat the shit out of my already overworked computer. So let’s shuffle the data and grab a subset of the examples.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>small_train_dataset <span class="op">=</span> tokenized_data[<span class="st">&quot;train&quot;</span>].shuffle(seed<span class="op">=</span><span class="dv">42</span>).select(<span class="bu">range</span>(<span class="dv">1000</span>))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>small_val_dataset <span class="op">=</span> tokenized_data[<span class="st">&quot;validation&quot;</span>].shuffle(seed<span class="op">=</span><span class="dv">42</span>).select(<span class="bu">range</span>(<span class="dv">100</span>))</span></code></pre></div>
<p>Now for the fun part - let’s build this model! First, we load BERT from a pretrained HuggingFace location and specify how many labels BERT will have.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertForSequenceClassification.from_pretrained(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;bert-base-uncased&quot;</span>, num_labels<span class="op">=</span><span class="dv">8</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<h4 id="training-eval"><strong>3. Training &amp; Evaluating</strong></h4>
<p>Second, we load the training arguments for the model (this could also be known as the config). <code>TrainingArguments</code> has a ton of parameters so you can check those out <a href="https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments">here</a>.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;bert_trainer&quot;</span>,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    evaluation_strategy<span class="op">=</span><span class="st">&quot;epoch&quot;</span>,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Finally, we’re ready to train! We set up an abstract Trainer class and give it our model, arguments, the training dataset, and the validation dataset to evaluate on. Calling the <code>trainer.train()</code> method (not surprisingly) kicks of the model fine-tuning.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>small_train_dataset,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>small_val_dataset,</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>trainer.evaluate()</span></code></pre></div>
<p>The first output will probably look something like…</p>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;]
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
<p>But don’t freak out! This is what we expect because we are randomly initializing the weights of the last head for our classification task.</p>
<p>If you have access to a GPU, HuggingFace will automatically find the device and push most calculations to that. I was able to run the entire dataset on a single Nvidia 2080 RTX Ti GPU in around 90 minutes. I recognize that not all people have access to such compute power, so for comparison, on a <code>2.4 GHz 8-Core Intel Core i9</code> processor, I was able to train on 1,000 examples in ~60 minutes.</p>
<p>Training on 1,000 examples, you should see a <code>micro f1</code> score of X%.</p>
<blockquote>
<p>Bonus question answer: the <a href="https://docs.python.org/3/library/pickle.html"><code>pickle</code></a> module, which is the default serializer in Python, does not serialize or deserialize code, e.g. lambda functions. It only serializes the names of classes/methods/functions. Therefore, if you want to save your model to use again, you cannot use an anonymous function.</p>
</blockquote>
<p>If you have any comments, questions, or corrections, feel free to drop me a line.</p>
<h4 id="thanks-to">Thanks to:</h4>
<p><a href="https://www.linkedin.com/in/dan-c-knight/">Dan Knight</a> for his feedback and encouragement.</p>
</body>
</html>
